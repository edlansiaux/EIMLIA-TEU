"""
√âvaluation des mod√®les de triage
================================

M√©triques et comparaison des performances.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any
from sklearn.metrics import (
    f1_score, cohen_kappa_score, mean_absolute_error,
    mean_squared_error, classification_report, confusion_matrix
)
from scipy.stats import spearmanr


def evaluate_model(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    model_name: str = "Model",
    verbose: bool = True
) -> Dict[str, float]:
    """
    √âvalue un mod√®le avec les m√©triques du protocole EIMLIA.
    
    M√©triques calcul√©es:
        - MAE: Mean Absolute Error (erreur moyenne en niveaux)
        - RMSE: Root Mean Square Error
        - Kappa: Cohen's Kappa pond√©r√© (accord inter-annotateur)
        - Spearman: Corr√©lation de rang
        - F1_micro/macro: F1-scores
        - Exact: % de pr√©dictions exactes
        - Near: % de pr√©dictions √† ¬±1 classe
    
    Args:
        y_true: Labels r√©els
        y_pred: Pr√©dictions
        model_name: Nom du mod√®le pour affichage
        verbose: Afficher les r√©sultats
        
    Returns:
        Dict des m√©triques
    """
    results = {
        'MAE': mean_absolute_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'Kappa': cohen_kappa_score(y_true, y_pred, weights='quadratic'),
        'Spearman': spearmanr(y_true, y_pred)[0],
        'F1_micro': f1_score(y_true, y_pred, average='micro'),
        'F1_macro': f1_score(y_true, y_pred, average='macro'),
        'Exact': float(np.mean(y_true == y_pred)),
        'Near': float(np.mean(np.abs(y_true - y_pred) <= 1))
    }
    
    # Taux d'erreur par type
    diff = y_pred - y_true
    results['Sous_triage'] = float(np.mean(diff < 0))  # Pr√©dit moins grave
    results['Sur_triage'] = float(np.mean(diff > 0))   # Pr√©dit plus grave
    
    if verbose:
        print(f"\n{'=' * 50}")
        print(f"üìä R√âSULTATS: {model_name}")
        print('=' * 50)
        for k, v in results.items():
            print(f"  {k:12}: {v:.4f}")
        
        # Classification report d√©taill√©
        print(f"\n  Classification Report:")
        print(classification_report(y_true, y_pred, digits=3))
    
    return results


def compare_models(
    results: Dict[str, Dict[str, float]],
    verbose: bool = True
) -> pd.DataFrame:
    """
    Compare plusieurs mod√®les et calcule un Z-score composite.
    
    Le Z-score combine:
        - MAE, RMSE (plus bas = meilleur, donc invers√©)
        - Kappa, Spearman (plus haut = meilleur)
    
    Args:
        results: Dict {model_name: metrics_dict}
        verbose: Afficher la comparaison
        
    Returns:
        DataFrame avec toutes les m√©triques et le Z-score
    """
    df = pd.DataFrame(results).T
    
    # Calculer Z-scores
    z_scores = {}
    for metric in ['MAE', 'RMSE', 'Kappa', 'Spearman']:
        if metric in df.columns:
            values = df[metric].values
            z = (values - values.mean()) / (values.std() + 1e-6)
            # Inverser pour MAE/RMSE (plus bas = meilleur)
            if metric in ['MAE', 'RMSE']:
                z = -z
            z_scores[metric] = z
    
    df['Z_composite'] = sum(z_scores.values())
    
    if verbose:
        print("\n" + "=" * 70)
        print("üìä COMPARAISON FINALE DES MOD√àLES")
        print("=" * 70)
        
        # Afficher tableau format√©
        display_cols = ['MAE', 'RMSE', 'Kappa', 'Spearman', 'Exact', 'Near', 'Z_composite']
        display_cols = [c for c in display_cols if c in df.columns]
        print(df[display_cols].round(4).to_string())
        
        # Ranking
        print("\nüèÜ RANKING (Z-score composite):")
        ranking = df['Z_composite'].sort_values(ascending=False)
        for i, (model, score) in enumerate(ranking.items(), 1):
            medal = ["ü•á", "ü•à", "ü•â"][i-1] if i <= 3 else f"{i}."
            print(f"  {medal} {model}: {score:.3f}")
    
    return df


def compute_confusion_matrix(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    class_names: List[str] = None,
    normalize: bool = True
) -> pd.DataFrame:
    """
    Calcule et formate la matrice de confusion.
    
    Args:
        y_true: Labels r√©els
        y_pred: Pr√©dictions
        class_names: Noms des classes
        normalize: Normaliser par ligne
        
    Returns:
        DataFrame de la matrice de confusion
    """
    cm = confusion_matrix(y_true, y_pred)
    
    if normalize:
        cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-6)
    
    if class_names is None:
        class_names = [f"Classe {i}" for i in range(cm.shape[0])]
    
    return pd.DataFrame(
        cm,
        index=[f"R√©el: {c}" for c in class_names],
        columns=[f"Pr√©dit: {c}" for c in class_names]
    )


def compute_triage_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    critical_classes: List[int] = None
) -> Dict[str, float]:
    """
    Calcule les m√©triques sp√©cifiques au triage m√©dical.
    
    Args:
        y_true: Labels r√©els (niveaux de gravit√©)
        y_pred: Pr√©dictions
        critical_classes: Classes consid√©r√©es comme critiques (d√©faut: [3, 4] = FRENCH 4-5)
        
    Returns:
        Dict avec m√©triques triage:
            - concordance: % accord exact
            - sous_triage: % de patients sous-class√©s
            - sur_triage: % de patients sur-class√©s
            - sous_triage_critique: % de patients critiques sous-class√©s (dangereux)
            - sensibilite_critique: sensibilit√© pour d√©tecter les cas critiques
            - specificite_critique: sp√©cificit√© pour les cas critiques
    """
    if critical_classes is None:
        critical_classes = [2, 3]  # Indices 0-based pour CCMU 3-4
    
    diff = y_pred - y_true
    
    # M√©triques de base
    concordance = float(np.mean(y_true == y_pred))
    sous_triage = float(np.mean(diff < 0))
    sur_triage = float(np.mean(diff > 0))
    
    # Sous-triage critique (patients graves class√©s moins graves)
    is_critical_real = np.isin(y_true, critical_classes)
    sous_triage_critique = 0.0
    if is_critical_real.sum() > 0:
        sous_triage_critique = float(np.mean(diff[is_critical_real] < 0))
    
    # Sensibilit√©/Sp√©cificit√© pour cas critiques
    is_critical_pred = np.isin(y_pred, critical_classes)
    
    # True positives (critiques d√©tect√©s)
    tp = np.sum(is_critical_real & is_critical_pred)
    # False negatives (critiques manqu√©s)
    fn = np.sum(is_critical_real & ~is_critical_pred)
    # False positives (non-critiques class√©s critiques)
    fp = np.sum(~is_critical_real & is_critical_pred)
    # True negatives
    tn = np.sum(~is_critical_real & ~is_critical_pred)
    
    sensibilite = tp / (tp + fn + 1e-6)
    specificite = tn / (tn + fp + 1e-6)
    
    return {
        'concordance': concordance,
        'sous_triage': sous_triage,
        'sur_triage': sur_triage,
        'sous_triage_critique': sous_triage_critique,
        'sensibilite_critique': float(sensibilite),
        'specificite_critique': float(specificite)
    }


def generate_report(
    results: Dict[str, Dict[str, float]],
    output_path: str = None
) -> str:
    """
    G√©n√®re un rapport textuel complet de comparaison.
    
    Args:
        results: Dict {model_name: metrics_dict}
        output_path: Chemin optionnel pour sauvegarder
        
    Returns:
        Rapport format√© en texte
    """
    lines = []
    lines.append("=" * 70)
    lines.append("RAPPORT D'√âVALUATION - √âTUDE EIMLIA-3M-TEU")
    lines.append("=" * 70)
    lines.append("")
    
    # R√©sum√© par mod√®le
    for model_name, metrics in results.items():
        lines.append(f"\n{'‚îÄ' * 50}")
        lines.append(f"üìä {model_name}")
        lines.append('‚îÄ' * 50)
        
        for metric, value in metrics.items():
            lines.append(f"  {metric:20}: {value:.4f}")
    
    # Comparaison
    df = compare_models(results, verbose=False)
    
    lines.append(f"\n{'=' * 70}")
    lines.append("CLASSEMENT FINAL")
    lines.append('=' * 70)
    
    ranking = df['Z_composite'].sort_values(ascending=False)
    for i, (model, score) in enumerate(ranking.items(), 1):
        lines.append(f"  {i}. {model}: Z={score:.3f}")
    
    # Recommandation
    best_model = ranking.index[0]
    lines.append(f"\n‚úÖ Recommandation: {best_model}")
    
    report = "\n".join(lines)
    
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"Rapport sauvegard√©: {output_path}")
    
    return report
